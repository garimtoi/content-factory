#!/usr/bin/env python
"""
ë©€í‹° ì¹´íƒˆë¡œê·¸ ìŠ¤í‚¤ë§ˆ ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸

í•˜ë‚˜ì˜ ì½˜í…ì¸ (file)ê°€ ì—¬ëŸ¬ ì¹´íƒˆë¡œê·¸ì— ì†í•  ìˆ˜ ìˆë„ë¡ N:N ê´€ê³„ í…Œì´ë¸”ì„ ìƒì„±í•©ë‹ˆë‹¤.

Usage:
    python scripts/migrate_multi_catalog.py --dry-run   # ì‹œë®¬ë ˆì´ì…˜
    python scripts/migrate_multi_catalog.py             # ì‹¤í–‰
    python scripts/migrate_multi_catalog.py --rollback  # ë¡¤ë°±
"""

import argparse
import sqlite3
import sys
from datetime import datetime
from pathlib import Path

# Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding="utf-8")

POKERVOD_DB = Path("D:/AI/claude01/qwen_hand_analysis/data/pokervod.db")

# ì‹ ê·œ í…Œì´ë¸” DDL
TABLES = {
    "file_catalogs": """
        CREATE TABLE IF NOT EXISTS file_catalogs (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            file_id VARCHAR(200) NOT NULL,
            catalog_id VARCHAR(50) NOT NULL,
            subcatalog_id VARCHAR(100),
            is_primary BOOLEAN DEFAULT FALSE,
            display_order INTEGER DEFAULT 0,
            added_by VARCHAR(50) DEFAULT 'system',
            added_reason VARCHAR(200),
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(file_id, catalog_id),
            FOREIGN KEY (file_id) REFERENCES files(id),
            FOREIGN KEY (catalog_id) REFERENCES catalogs(id),
            FOREIGN KEY (subcatalog_id) REFERENCES subcatalogs(id)
        )
    """,
    "catalog_collections": """
        CREATE TABLE IF NOT EXISTS catalog_collections (
            id VARCHAR(100) PRIMARY KEY,
            name VARCHAR(200) NOT NULL,
            description TEXT,
            collection_type VARCHAR(50) NOT NULL DEFAULT 'curated',
            cover_image_url TEXT,
            is_dynamic BOOLEAN DEFAULT FALSE,
            filter_query JSON,
            display_order INTEGER DEFAULT 0,
            is_active BOOLEAN DEFAULT TRUE,
            created_by VARCHAR(50),
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """,
    "collection_items": """
        CREATE TABLE IF NOT EXISTS collection_items (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            collection_id VARCHAR(100) NOT NULL,
            file_id VARCHAR(200) NOT NULL,
            display_order INTEGER DEFAULT 0,
            added_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(collection_id, file_id),
            FOREIGN KEY (collection_id) REFERENCES catalog_collections(id),
            FOREIGN KEY (file_id) REFERENCES files(id)
        )
    """,
}

INDEXES = {
    "file_catalogs": [
        "CREATE INDEX IF NOT EXISTS idx_file_catalogs_file ON file_catalogs(file_id)",
        "CREATE INDEX IF NOT EXISTS idx_file_catalogs_catalog ON file_catalogs(catalog_id)",
        "CREATE INDEX IF NOT EXISTS idx_file_catalogs_subcatalog ON file_catalogs(subcatalog_id)",
        "CREATE INDEX IF NOT EXISTS idx_file_catalogs_primary ON file_catalogs(is_primary)",
    ],
    "collection_items": [
        "CREATE INDEX IF NOT EXISTS idx_collection_items_collection ON collection_items(collection_id)",
        "CREATE INDEX IF NOT EXISTS idx_collection_items_file ON collection_items(file_id)",
    ],
}

# ê¸°ë³¸ ì»¬ë ‰ì…˜ ë°ì´í„°
DEFAULT_COLLECTIONS = [
    ("highlights", "ë² ìŠ¤íŠ¸ í•¸ë“œ", "í•˜ì´ë¼ì´íŠ¸ ì ìˆ˜ 3ì  í•¸ë“œ ëª¨ìŒ", "curated", None, False, None, 1, True, "system"),
    ("epic-hands", "ì—­ëŒ€ê¸‰ í•¸ë“œ", "ì „ì„¤ì ì¸ í•¸ë“œ ì»¬ë ‰ì…˜", "curated", None, False, None, 2, True, "system"),
    ("player-phil-ivey", "Phil Ivey", "Phil Ivey ë“±ì¥ ì˜ìƒ", "player", None, True, '{"player": "Phil Ivey"}', 10, True, "system"),
    ("player-tom-dwan", "Tom Dwan", "Tom Dwan ë“±ì¥ ì˜ìƒ", "player", None, True, '{"player": "Tom Dwan"}', 11, True, "system"),
    ("player-daniel-negreanu", "Daniel Negreanu", "Daniel Negreanu ë“±ì¥ ì˜ìƒ", "player", None, True, '{"player": "Daniel Negreanu"}', 12, True, "system"),
    ("tag-bluff", "ë¸”ëŸ¬í”„ ëª…ì¥ë©´", "ë¸”ëŸ¬í”„ íƒœê·¸ í•¸ë“œ", "tag", None, True, '{"tag": "bluff"}', 20, True, "system"),
    ("tag-cooler", "ì¿¨ëŸ¬ í•¸ë“œ", "ì¿¨ëŸ¬/ë°°ë“œë¹— ìƒí™©", "tag", None, True, '{"tag": "cooler"}', 21, True, "system"),
    ("tag-allin", "ì˜¬ì¸ ëª…ìŠ¹ë¶€", "í”„ë¦¬í”Œë/ë©€í‹°ì›¨ì´ ì˜¬ì¸", "tag", None, True, '{"tags": ["preflop_allin", "multiway_allin"]}', 22, True, "system"),
    ("recent-week", "ì´ë²ˆ ì£¼ ì—…ë¡œë“œ", "ìµœê·¼ 7ì¼ ì—…ë¡œë“œ ì½˜í…ì¸ ", "dynamic", None, True, '{"days": 7}', 30, True, "system"),
    ("most-viewed", "ê°€ì¥ ë§ì´ ë³¸ ì˜ìƒ", "ì¡°íšŒìˆ˜ Top 100", "dynamic", None, True, '{"sort": "view_count", "limit": 100}', 31, True, "system"),
]


def get_existing_tables(conn: sqlite3.Connection) -> set:
    """ê¸°ì¡´ í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ"""
    cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table'")
    return {row[0] for row in cursor.fetchall()}


def migrate_existing_data(conn: sqlite3.Connection, dry_run: bool = False) -> int:
    """ê¸°ì¡´ íŒŒì¼ë“¤ì˜ ì¹´íƒˆë¡œê·¸ ê´€ê³„ë¥¼ file_catalogsë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜"""

    # ê¸°ì¡´ ê³„ì¸µ êµ¬ì¡°ì—ì„œ file â†’ catalog ê´€ê³„ ì¶”ì¶œ
    query = """
        SELECT DISTINCT
            f.id as file_id,
            c.id as catalog_id,
            s.id as subcatalog_id
        FROM files f
        JOIN events e ON f.event_id = e.id
        JOIN tournaments t ON e.tournament_id = t.id
        JOIN subcatalogs s ON t.subcatalog_id = s.id
        JOIN catalogs c ON s.catalog_id = c.id
        WHERE f.id IS NOT NULL AND c.id IS NOT NULL
    """

    cursor = conn.execute(query)
    rows = cursor.fetchall()

    if dry_run:
        return len(rows)

    # file_catalogsì— ì‚½ì… (is_primary=TRUEë¡œ ì›ë³¸ ê´€ê³„ í‘œì‹œ)
    conn.executemany(
        """
        INSERT OR IGNORE INTO file_catalogs
        (file_id, catalog_id, subcatalog_id, is_primary, added_by, added_reason)
        VALUES (?, ?, ?, TRUE, 'migration', 'Original catalog from hierarchy')
        """,
        rows
    )

    return len(rows)


def migrate(conn: sqlite3.Connection, dry_run: bool = False) -> dict:
    """ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰"""
    results = {
        "tables_created": [],
        "tables_skipped": [],
        "indexes_created": [],
        "data_migrated": 0,
        "collections_created": 0,
    }

    existing_tables = get_existing_tables(conn)

    # 1. í…Œì´ë¸” ìƒì„±
    for table_name, ddl in TABLES.items():
        if table_name in existing_tables:
            results["tables_skipped"].append(table_name)
            print(f"â­ï¸  {table_name}: ì´ë¯¸ ì¡´ì¬ (ìŠ¤í‚µ)")
        else:
            if dry_run:
                print(f"ğŸ” {table_name}: ìƒì„± ì˜ˆì •")
            else:
                conn.execute(ddl)
                print(f"âœ… {table_name}: ìƒì„± ì™„ë£Œ")
            results["tables_created"].append(table_name)

    # 2. ì¸ë±ìŠ¤ ìƒì„±
    for table_name, index_ddls in INDEXES.items():
        for index_ddl in index_ddls:
            index_name = index_ddl.split("IF NOT EXISTS ")[1].split(" ON")[0]
            if dry_run:
                print(f"ğŸ” {index_name}: ìƒì„± ì˜ˆì •")
            else:
                conn.execute(index_ddl)
                print(f"âœ… {index_name}: ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
            results["indexes_created"].append(index_name)

    # 3. ê¸°ì¡´ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜
    if "file_catalogs" in results["tables_created"]:
        count = migrate_existing_data(conn, dry_run)
        results["data_migrated"] = count
        if dry_run:
            print(f"ğŸ” file_catalogs: {count}ê°œ ê¸°ì¡´ ê´€ê³„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì˜ˆì •")
        else:
            print(f"âœ… file_catalogs: {count}ê°œ ê¸°ì¡´ ê´€ê³„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ")

    # 4. ê¸°ë³¸ ì»¬ë ‰ì…˜ ë°ì´í„° ì‚½ì…
    if "catalog_collections" in results["tables_created"]:
        if dry_run:
            print(f"ğŸ” catalog_collections: {len(DEFAULT_COLLECTIONS)}ê°œ ê¸°ë³¸ ì»¬ë ‰ì…˜ ìƒì„± ì˜ˆì •")
        else:
            conn.executemany(
                """
                INSERT INTO catalog_collections
                (id, name, description, collection_type, cover_image_url,
                 is_dynamic, filter_query, display_order, is_active, created_by)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                DEFAULT_COLLECTIONS
            )
            print(f"âœ… catalog_collections: {len(DEFAULT_COLLECTIONS)}ê°œ ê¸°ë³¸ ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ")
        results["collections_created"] = len(DEFAULT_COLLECTIONS)

    if not dry_run:
        conn.commit()

    return results


def rollback(conn: sqlite3.Connection, dry_run: bool = False) -> dict:
    """ë¡¤ë°± ì‹¤í–‰"""
    results = {"tables_dropped": [], "tables_not_found": []}
    existing_tables = get_existing_tables(conn)

    for table_name in reversed(list(TABLES.keys())):
        if table_name in existing_tables:
            if dry_run:
                print(f"ğŸ” {table_name}: ì‚­ì œ ì˜ˆì •")
            else:
                conn.execute(f"DROP TABLE IF EXISTS {table_name}")
                print(f"ğŸ—‘ï¸  {table_name}: ì‚­ì œ ì™„ë£Œ")
            results["tables_dropped"].append(table_name)
        else:
            results["tables_not_found"].append(table_name)
            print(f"â­ï¸  {table_name}: ì¡´ì¬í•˜ì§€ ì•ŠìŒ (ìŠ¤í‚µ)")

    if not dry_run:
        conn.commit()

    return results


def verify(conn: sqlite3.Connection) -> dict:
    """ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦"""
    results = {"existing": [], "missing": [], "row_counts": {}, "sample_data": []}
    existing_tables = get_existing_tables(conn)

    for table_name in TABLES.keys():
        if table_name in existing_tables:
            results["existing"].append(table_name)
            cursor = conn.execute(f"SELECT COUNT(*) FROM {table_name}")
            count = cursor.fetchone()[0]
            results["row_counts"][table_name] = count
        else:
            results["missing"].append(table_name)

    # ë©€í‹° ì¹´íƒˆë¡œê·¸ ìƒ˜í”Œ í™•ì¸
    if "file_catalogs" in existing_tables:
        cursor = conn.execute("""
            SELECT fc.file_id, GROUP_CONCAT(fc.catalog_id, ', ') as catalogs, COUNT(*) as cnt
            FROM file_catalogs fc
            GROUP BY fc.file_id
            HAVING cnt > 1
            LIMIT 5
        """)
        results["sample_data"] = cursor.fetchall()

    return results


def show_stats(conn: sqlite3.Connection):
    """ë©€í‹° ì¹´íƒˆë¡œê·¸ í†µê³„ ì¶œë ¥"""
    print("\nğŸ“Š ë©€í‹° ì¹´íƒˆë¡œê·¸ í†µê³„\n")

    # ì¹´íƒˆë¡œê·¸ë³„ íŒŒì¼ ìˆ˜
    cursor = conn.execute("""
        SELECT catalog_id, COUNT(*) as cnt
        FROM file_catalogs
        GROUP BY catalog_id
        ORDER BY cnt DESC
    """)
    print("ì¹´íƒˆë¡œê·¸ë³„ íŒŒì¼ ìˆ˜:")
    for row in cursor.fetchall():
        print(f"  {row[0]}: {row[1]:,}ê°œ")

    print()

    # ë©€í‹° ì¹´íƒˆë¡œê·¸ íŒŒì¼ ìˆ˜
    cursor = conn.execute("""
        SELECT COUNT(*) FROM (
            SELECT file_id FROM file_catalogs GROUP BY file_id HAVING COUNT(*) > 1
        )
    """)
    multi_count = cursor.fetchone()[0]

    cursor = conn.execute("SELECT COUNT(DISTINCT file_id) FROM file_catalogs")
    total_count = cursor.fetchone()[0]

    print(f"ë©€í‹° ì¹´íƒˆë¡œê·¸ íŒŒì¼: {multi_count:,}ê°œ / {total_count:,}ê°œ")

    # ì»¬ë ‰ì…˜ í†µê³„
    cursor = conn.execute("""
        SELECT collection_type, COUNT(*)
        FROM catalog_collections
        GROUP BY collection_type
    """)
    print("\nì»¬ë ‰ì…˜ ìœ í˜•ë³„:")
    for row in cursor.fetchall():
        print(f"  {row[0]}: {row[1]}ê°œ")


def main():
    parser = argparse.ArgumentParser(description="ë©€í‹° ì¹´íƒˆë¡œê·¸ ìŠ¤í‚¤ë§ˆ ë§ˆì´ê·¸ë ˆì´ì…˜")
    parser.add_argument("--dry-run", action="store_true", help="ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ")
    parser.add_argument("--rollback", action="store_true", help="ë¡¤ë°± ëª¨ë“œ")
    parser.add_argument("--verify", action="store_true", help="ê²€ì¦ ëª¨ë“œ")
    parser.add_argument("--stats", action="store_true", help="í†µê³„ ì¶œë ¥")
    parser.add_argument("--db-path", type=str, default=str(POKERVOD_DB), help="DB ê²½ë¡œ")

    args = parser.parse_args()

    db_path = Path(args.db_path)
    if not db_path.exists():
        print(f"âŒ DB íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {db_path}")
        sys.exit(1)

    print(f"ğŸ“ DB: {db_path}")
    print(f"ğŸ“… ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("-" * 50)

    conn = sqlite3.connect(str(db_path))

    try:
        if args.stats:
            show_stats(conn)
        elif args.verify:
            print("ğŸ” ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒíƒœ ê²€ì¦\n")
            results = verify(conn)

            print(f"\nâœ… ì¡´ì¬í•˜ëŠ” í…Œì´ë¸”: {len(results['existing'])}/{len(TABLES)}")
            for table in results["existing"]:
                count = results["row_counts"].get(table, 0)
                print(f"   - {table}: {count:,} rows")

            if results["missing"]:
                print(f"\nâŒ ëˆ„ë½ëœ í…Œì´ë¸”: {len(results['missing'])}")
                for table in results["missing"]:
                    print(f"   - {table}")

            if results["sample_data"]:
                print(f"\nğŸ”— ë©€í‹° ì¹´íƒˆë¡œê·¸ íŒŒì¼ ìƒ˜í”Œ:")
                for row in results["sample_data"]:
                    print(f"   - File {row[0]}: [{row[1]}] ({row[2]}ê°œ ì¹´íƒˆë¡œê·¸)")

        elif args.rollback:
            print(f"{'ğŸ” ë¡¤ë°± ì‹œë®¬ë ˆì´ì…˜' if args.dry_run else 'ğŸ—‘ï¸  ë¡¤ë°± ì‹¤í–‰'}\n")
            results = rollback(conn, dry_run=args.dry_run)
            print(f"\nğŸ“Š ê²°ê³¼:")
            print(f"   ì‚­ì œëœ í…Œì´ë¸”: {len(results['tables_dropped'])}")

        else:
            print(f"{'ğŸ” ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œë®¬ë ˆì´ì…˜' if args.dry_run else 'ğŸš€ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰'}\n")
            results = migrate(conn, dry_run=args.dry_run)

            print(f"\nğŸ“Š ê²°ê³¼:")
            print(f"   ìƒì„±ëœ í…Œì´ë¸”: {len(results['tables_created'])}")
            print(f"   ìŠ¤í‚µëœ í…Œì´ë¸”: {len(results['tables_skipped'])}")
            print(f"   ìƒì„±ëœ ì¸ë±ìŠ¤: {len(results['indexes_created'])}")
            print(f"   ë§ˆì´ê·¸ë ˆì´ì…˜ëœ ê´€ê³„: {results['data_migrated']:,}ê°œ")
            print(f"   ìƒì„±ëœ ì»¬ë ‰ì…˜: {results['collections_created']}ê°œ")

    finally:
        conn.close()

    print("\nâœ¨ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
